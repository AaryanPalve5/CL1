{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d458e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assignment ML6: Tic-Tac-Toe with Q-learning (Reinforcement Learning)\n",
    "# Educational, simple Q-learning agent vs a random opponent. Not optimized for speed.\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Board representation: 0 empty, 1 agent (X), -1 opponent (O)\n",
    "def available_actions(state):\n",
    "    return [i for i, v in enumerate(state) if v == 0]\n",
    "\n",
    "def check_winner(state):\n",
    "    board = np.array(state).reshape(3,3)\n",
    "    for s in [1, -1]:\n",
    "        # rows, cols, diags\n",
    "        if any(np.all(board[r,:]==s) for r in range(3)): return s\n",
    "        if any(np.all(board[:,c]==s) for c in range(3)): return s\n",
    "        if np.all(np.diag(board)==s) or np.all(np.diag(np.fliplr(board))==s): return s\n",
    "    if 0 not in state: return 0  # draw\n",
    "    return None\n",
    "\n",
    "def state_to_key(state):\n",
    "    return tuple(state)\n",
    "\n",
    "# Q-table as defaultdict\n",
    "Q = defaultdict(float)\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.2\n",
    "\n",
    "def choose_action(state):\n",
    "    actions = available_actions(state)\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)\n",
    "    # choose action with max Q-value\n",
    "    qs = [Q[(state_to_key(state), a)] for a in actions]\n",
    "    max_q = max(qs)\n",
    "    max_actions = [a for a,q in zip(actions, qs) if q == max_q]\n",
    "    return random.choice(max_actions)\n",
    "\n",
    "def play_episode(train=True):\n",
    "    state = [0]*9\n",
    "    history = []\n",
    "    while True:\n",
    "        # agent move\n",
    "        a = choose_action(state)\n",
    "        state[a] = 1\n",
    "        history.append((state_to_key(state), a))\n",
    "        winner = check_winner(state)\n",
    "        if winner is not None:\n",
    "            reward = 1 if winner==1 else (0 if winner==0 else -1)\n",
    "            break\n",
    "        # opponent random move\n",
    "        opp_actions = available_actions(state)\n",
    "        if not opp_actions:\n",
    "            winner = check_winner(state)\n",
    "            reward = 1 if winner==1 else (0 if winner==0 else -1)\n",
    "            break\n",
    "        opp = random.choice(opp_actions)\n",
    "        state[opp] = -1\n",
    "        winner = check_winner(state)\n",
    "        if winner is not None:\n",
    "            reward = 1 if winner==1 else (0 if winner==0 else -1)\n",
    "            break\n",
    "    # Q-learning update for history (simple Monte-Carlo-like update)\n",
    "    if train:\n",
    "        for s,a in reversed(history):\n",
    "            old = Q[(s,a)]\n",
    "            Q[(s,a)] = old + alpha*(reward - old)\n",
    "            reward = reward*gamma\n",
    "    return winner\n",
    "\n",
    "# Training\n",
    "episodes = 5000\n",
    "wins = {1:0, -1:0, 0:0}\n",
    "for ep in range(episodes):\n",
    "    w = play_episode(train=True)\n",
    "    wins[w] += 1\n",
    "    if (ep+1) % 1000 == 0:\n",
    "        print(f\"Episode {ep+1}, cumulative wins: {wins}\")\n",
    "\n",
    "print(\"Training complete. Stats:\", wins)\n",
    "\n",
    "# Evaluate agent vs random opponent\n",
    "test_episodes = 1000\n",
    "wins = {1:0, -1:0, 0:0}\n",
    "for _ in range(test_episodes):\n",
    "    w = play_episode(train=False)\n",
    "    wins[w] += 1\n",
    "print(\"Evaluation results (agent win=1, draw=0, loss=-1):\", wins)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
